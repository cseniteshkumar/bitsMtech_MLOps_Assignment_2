{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b6497f2",
   "metadata": {},
   "source": [
    "Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f53bf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "# sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "experiment_name = 'CNN_Model_Training_Experiment'\n",
    "\n",
    "mlflow_dir = \"./mlruns\"\n",
    "\n",
    "mlflow_dir = Path(mlflow_dir)\n",
    "\n",
    "if not os.path.exists(mlflow_dir):\n",
    "    os.makedirs(mlflow_dir)\n",
    "\n",
    "mlflow.set_tracking_uri(f\"file://{os.path.abspath(mlflow_dir)}\")\n",
    "\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "\n",
    "mlflow.autolog()\n",
    "mlflow.sklearn.autolog(log_models=True, log_datasets=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559ea453-6e0f-4ed7-ac74-5ae86ac0390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if a GPU is available\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Get the name of the device\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51b23e2-635b-4706-8b1a-d3e4f55f3cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n",
    "\n",
    "# x = torch.tensor([1.0, 2.0, 3.0]).to(device)\n",
    "\n",
    "# x = torch.tensor([1.0, 2.0, 3.0]).to(device)\n",
    "\n",
    "# # Any operation now uses CUDA cores\n",
    "# y = x * 2\n",
    "# print(y)\n",
    "import tensorflow as tf\n",
    "tf.config.optimizer.set_jit(True) # Enables XLA globally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecabca33-3649-400d-89b0-9565b5652811",
   "metadata": {},
   "source": [
    "Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eef6da2-4058-4475-adfe-96f3728a1075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "# 1. Device Agnostic Setup (Updated for CUDA and Apple Silicon)\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available() \n",
    "    else \"mps\" if torch.backends.mps.is_available() \n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2. Modern Data Augmentation (Using Recommended Transforms)\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f06a9f-8ece-4e6b-b2ef-ef21298b5536",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001 # learning_rate\n",
    "batch_size = 100 # we will use mini-batch method\n",
    "epochs = 10 # How much to train a model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c8991d-3a84-46b0-a35d-fce995a62eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Modern Device Selection (Includes Apple Silicon Support)\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available() \n",
    "    else \"mps\" if torch.backends.mps.is_available() \n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "# 2. Comprehensive Global Seeding\n",
    "seed = 1234\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed) # Safe to call even if cuda isn't available\n",
    "np.random.seed(seed)            # Good practice if using NumPy for data prep\n",
    "\n",
    "# 3. Optional: Ensure Deterministic Behavior (for strict reproducibility)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(f\"Active Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5f2a03-2b6d-4f5f-8f7b-abfcb42e153e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Save datasets path\n",
    "# train_image_path = \"DataFiles/SampleFiles/train/\"\n",
    "# test_image_path = \"DataFiles/SampleFiles/test/\"\n",
    "\n",
    "# TRAIN_FOLDER = \"DataFiles/SampleFiles/train/\"\n",
    "# TEST_FOLDER =  \"DataFiles/SampleFiles/test/\"\n",
    "\n",
    "TRAIN_FOLDER = \"DataFiles/train/\"\n",
    "TEST_FOLDER =  \"DataFiles/test/\"\n",
    "\n",
    "# train_image_list = os.listdir(train_image_path)[0:SAMPLE_SIZE]\n",
    "train_image_list = os.listdir(TRAIN_FOLDER)\n",
    "test_image_list = os.listdir(TEST_FOLDER)\n",
    "\n",
    "train_image_list = [TRAIN_FOLDER+x for x in train_image_list]\n",
    "test_image_list = [TEST_FOLDER+x for x in test_image_list]\n",
    "\n",
    "print(len(train_image_list))\n",
    "print(len(test_image_list))\n",
    "\n",
    "# os.listdir(TRAIN_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a12cd9-e0d1-46c0-bfe5-9d05bb2fa6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# 1. Modern Subplot Generation\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
    "axes = axes.flatten()  # Flatten the 2x5 array into a 1D list for easy iteration\n",
    "\n",
    "# 2. Random Index Selection\n",
    "random_indices = np.random.choice(len(train_image_list), size=10, replace=False)\n",
    "\n",
    "# 3. Clean Enumerated Loop\n",
    "for i, idx in enumerate(random_indices):\n",
    "    img = Image.open(train_image_list[idx])\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].axis('off')  # Turn off axis for each specific subplot\n",
    "    # Optional: axes[i].set_title(f\"Index: {idx}\", fontsize=8)\n",
    "\n",
    "plt.tight_layout() # Automatically adjusts spacing\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c94cde6-a053-4450-878f-89cc822fa55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Ensure reproducibility with a fixed seed\n",
    "# 2. Use stratify if you have imbalanced classes (optional)\n",
    "train_list, val_list = train_test_split(\n",
    "    train_image_list, \n",
    "    test_size=0.2, \n",
    "    random_state=1234,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(train_list)}\")\n",
    "print(f\"Val size: {len(val_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caafcf6-78c2-410e-aeb4-432ee48f66e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageNet standard normalization values\n",
    "norm_mean = [0.485, 0.456, 0.406]\n",
    "norm_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# 1. Training: High Variety\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),      # Resize slightly larger first\n",
    "    transforms.RandomResizedCrop(224),  # Zoom in on different parts\n",
    "    transforms.RandomHorizontalFlip(),  # Flip left/right\n",
    "    transforms.ColorJitter(0.1, 0.1),   # Optional: minor color tweaks\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std)\n",
    "])\n",
    "\n",
    "# 2. Validation/Test: Consistency is Key\n",
    "# No random crops or flips here!\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),      # Just a clean resize\n",
    "    transforms.CenterCrop(224),        # Ensure the center object is visible\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c224384d-cdbd-4749-877a-95615c5b77f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, file_list, transform=None, is_test=False):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.file_list[idx]\n",
    "        \n",
    "        # 1. Open and ensure RGB\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # 2. Apply transforms\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        # 3. Extract ID or Label\n",
    "        filename = Path(img_path).name.lower()\n",
    "        \n",
    "        if self.is_test:\n",
    "            # For test data, extract numeric ID from filename (e.g., \"123.jpg\" -> 123)\n",
    "            file_id = int(filename.split('.')[0])\n",
    "            return img, file_id\n",
    "        else:\n",
    "            # For train/val data, extract label\n",
    "            if 'dog' in filename:\n",
    "                label = 1\n",
    "            elif 'cat' in filename:\n",
    "                label = 0\n",
    "            else:\n",
    "                label = -1 \n",
    "            return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240aa945-0a8d-43dd-b072-93b81f0368f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = ImageDataset(train_list, transform=train_transforms, is_test=False)\n",
    "val_data   = ImageDataset(val_list,   transform=val_test_transforms, is_test=False)\n",
    "test_data  = ImageDataset(test_image_list, transform=val_test_transforms, is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cae04be-3e1e-4e97-982d-1c1ad620abcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 1. Use all available CPU cores for data loading\n",
    "# This prevents the 'bottleneck' where the GPU waits for the CPU to resize images.\n",
    "num_cpus = os.cpu_count()\n",
    "\n",
    "# 2. Modern DataLoader Configuration\n",
    "loader_args = {\n",
    "    'batch_size': batch_size,\n",
    "    'num_workers': num_cpus,\n",
    "    'pin_memory': True if device != 'cpu' else False\n",
    "}\n",
    "\n",
    "# Training: Needs shuffling to prevent the model from learning the order\n",
    "train_loader = DataLoader(train_data, shuffle=True, **loader_args)\n",
    "\n",
    "# Validation & Test: Shuffle=False for consistent evaluation\n",
    "val_loader = DataLoader(val_data, shuffle=False, **loader_args)\n",
    "test_loader = DataLoader(test_data, shuffle=False, **loader_args)\n",
    "\n",
    "print(f\"Loaders ready using {num_cpus} CPU workers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b4bf1b-d7bd-4038-9a0d-4c52095cbe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Cnn, self).__init__()\n",
    "        \n",
    "        # Layer 1: 224 -> 55 -> 27\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1), # Added padding=1\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        \n",
    "        # Layer 2: 27 -> 7 -> 3\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        \n",
    "        # Layer 3: 3 -> 1\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            # MaxPool here might make the feature map too small (0 or 1)\n",
    "        )\n",
    "        \n",
    "        # Global Average Pooling is more modern than fixed Linear sizes\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        self.fc1 = nn.Linear(64, 10)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(10, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        \n",
    "        # Modern pooling: ensures the output is always the right size for FC layers\n",
    "        out = self.adaptive_pool(out)\n",
    "        out = torch.flatten(out, 1) # Cleaner than out.view()\n",
    "        \n",
    "        out = self.relu(self.fc1(out))\n",
    "        out = self.dropout(out) # Apply dropout during forward pass\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = Cnn().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47964de0-40c4-484e-bfac-5d07145c5ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Cnn().to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7101c3a7-b817-4e46-9d06-dc71a684d969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Modern Optimizer (AdamW is generally preferred over Adam for better weight decay)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "# 2. Loss Function (Standard for classification)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 3. Learning Rate Scheduler (Optional but recommended for \"latest\" performance)\n",
    "# Reduces the learning rate if the validation loss stops improving\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n",
    "\n",
    "print(f\"Model ready on {device}. Hyperparameters set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c806e9f-abf0-4291-80e0-612cf5fe4c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "patience = 3\n",
    "min_delta = 0.0005\n",
    "best_val_loss = float(\"inf\")\n",
    "early_stop_counter = 0\n",
    "\n",
    "# Track metrics per epoch\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": []\n",
    "}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # --- TRAINING PHASE ---\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    for data, label in train_loader:\n",
    "        data, label = data.to(device), label.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "        train_acc += (output.argmax(1) == label).sum().item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "    avg_train_acc = train_acc / len(train_loader.dataset)\n",
    "\n",
    "    # --- VALIDATION PHASE ---\n",
    "    model.eval()\n",
    "    val_loss, val_acc = 0, 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for data, label in val_loader:\n",
    "            data, label = data.to(device), label.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            loss = criterion(output, label)\n",
    "\n",
    "            val_loss += loss.item() * data.size(0)\n",
    "            val_acc += (output.argmax(1) == label).sum().item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "    avg_val_acc = val_acc / len(val_loader.dataset)\n",
    "\n",
    "    # Store history\n",
    "    history[\"train_loss\"].append(avg_train_loss)\n",
    "    history[\"train_acc\"].append(avg_train_acc)\n",
    "    history[\"val_loss\"].append(avg_val_loss)\n",
    "    history[\"val_acc\"].append(avg_val_acc)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}]\")\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {avg_train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {avg_val_acc:.4f}\\n\")\n",
    "\n",
    "    # --- EARLY STOPPING ---\n",
    "    if avg_val_loss < best_val_loss - min_delta:\n",
    "        best_val_loss = avg_val_loss\n",
    "        early_stop_counter = 0\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a510ba89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot after training\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history[\"train_acc\"], label=\"Train Acc\")\n",
    "plt.plot(history[\"val_acc\"], label=\"Val Acc\")\n",
    "plt.title(\"Accuracy per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "plt.plot(history[\"val_loss\"], label=\"Val Loss\")\n",
    "plt.title(\"Loss per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addfaef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model (recommended: state_dict)\n",
    "model_save_path = \"artifacts/cnn_model.pt\"\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to: {model_save_path}\")\n",
    "\n",
    "# (Optional) Save full model for quick reload without class definition changes\n",
    "full_model_path = \"artifacts/cnn_model_full.pt\"\n",
    "torch.save(model, full_model_path)\n",
    "print(f\"Full model saved to: {full_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437c7b6e-dcf5-4d64-9137-b0d13c8a7a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "dog_probs = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for data, file_id in test_loader:\n",
    "        data = data.to(device, non_blocking=True)\n",
    "        \n",
    "        logits = model(data)\n",
    "        probs = logits.softmax(dim=1)[:, 1].cpu().numpy()\n",
    "        \n",
    "        # file_id is already a tensor of integers\n",
    "        file_ids = file_id.cpu().numpy()\n",
    "        \n",
    "        # Zip the actual file IDs with probabilities\n",
    "        dog_probs.extend(zip(file_ids, probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7318fcbc-a3d4-47bd-a96e-8a7a5deb37b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_probs.sort(key = lambda x : int(x[0]))\n",
    "dog_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c55e4aa-011c-4df2-b476-f644b51a5d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [item[0] for item in dog_probs]\n",
    "prob = [item[1] for item in dog_probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8e611a-5bd9-4622-a154-da5fa76c041b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFrame directly from the zipped list of tuples\n",
    "submission = pd.DataFrame(dog_probs, columns=['id', 'label'])\n",
    "\n",
    "# Ensure 'id' is an integer (common for image filenames like '123.jpg')\n",
    "# and 'label' is a float32 to save memory\n",
    "submission['id'] = submission['id'].astype(int)\n",
    "submission['label'] = submission['label'].astype('float32')\n",
    "\n",
    "# Sort by ID to ensure a consistent submission format\n",
    "submission.sort_values('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe787f1a-ba21-4024-b086-da213979056f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19206904-5305-41d8-8d9b-70786c9ac6b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a55ea7-13ab-46e6-b712-faf2e58eb07f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c495fea-bacc-4f4d-8dc3-ac1e50ac2dab",
   "metadata": {},
   "source": [
    "## For Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99325ffc-4e82-433f-9fd1-aa5ed7872497",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================\n",
    "# 2. Setup Device and Transforms\n",
    "# ============================================\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available() \n",
    "    else \"mps\" if torch.backends.mps.is_available() \n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Same normalization as training\n",
    "norm_mean = [0.485, 0.456, 0.406]\n",
    "norm_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std)\n",
    "])\n",
    "\n",
    "# ============================================\n",
    "# 3. Load Model from Checkpoint\n",
    "# ============================================\n",
    "def load_model(model_path, device):\n",
    "    \"\"\"Load model weights from checkpoint\"\"\"\n",
    "    model = Cnn().to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# ============================================\n",
    "# 4. Prediction Function\n",
    "# ============================================\n",
    "def predict_single_image(image_path, model, device, transforms):\n",
    "    \"\"\"\n",
    "    Predict if image is a dog or cat\n",
    "    Returns: (prediction, probability)\n",
    "    \"\"\"\n",
    "    # Load and preprocess image\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_tensor = transforms(img).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Inference\n",
    "    with torch.inference_mode():\n",
    "        logits = model(img_tensor)\n",
    "        probs = logits.softmax(dim=1)\n",
    "        \n",
    "        # Get predictions\n",
    "        pred_class = probs.argmax(dim=1).item()\n",
    "        confidence = probs[0, pred_class].item()\n",
    "        dog_probability = probs[0, 1].item()\n",
    "    \n",
    "    # Map classes\n",
    "    class_names = {0: \"Cat\", 1: \"Dog\"}\n",
    "    \n",
    "    return {\n",
    "        \"image_path\": image_path,\n",
    "        \"prediction\": class_names[pred_class],\n",
    "        \"confidence\": confidence,\n",
    "        \"dog_probability\": dog_probability,\n",
    "        \"cat_probability\": probs[0, 0].item()\n",
    "    }\n",
    "\n",
    "# ============================================\n",
    "# 5. Batch Prediction Function\n",
    "# ============================================\n",
    "def predict_batch(image_folder, model, device, transforms):\n",
    "    \"\"\"Predict on all images in a folder\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    image_files = [f for f in os.listdir(image_folder) \n",
    "                   if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    for img_file in image_files:\n",
    "        img_path = os.path.join(image_folder, img_file)\n",
    "        try:\n",
    "            result = predict_single_image(img_path, model, device, transforms)\n",
    "            results.append(result)\n",
    "            print(f\"✓ {img_file}: {result['prediction']} ({result['confidence']:.2%})\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ {img_file}: Error - {str(e)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ============================================\n",
    "# 6. Main Execution\n",
    "# ============================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Load trained model\n",
    "    model_path = \"artifacts/cnn_model.pt\"\n",
    "    model = load_model(model_path, device)\n",
    "    print(f\"Model loaded from: {model_path}\\n\")\n",
    "    \n",
    "    # Example 1: Predict single image\n",
    "    print(\"=\" * 50)\n",
    "    print(\"SINGLE IMAGE PREDICTION\")\n",
    "    print(\"=\" * 50)\n",
    "    single_image_path = \"DataFiles/test/1.jpg\"  # Change this to your image\n",
    "    \n",
    "    if os.path.exists(single_image_path):\n",
    "        result = predict_single_image(single_image_path, model, device, val_test_transforms)\n",
    "        print(f\"\\nImage: {result['image_path']}\")\n",
    "        print(f\"Prediction: {result['prediction']}\")\n",
    "        print(f\"Confidence: {result['confidence']:.4f}\")\n",
    "        print(f\"Dog Probability: {result['dog_probability']:.4f}\")\n",
    "        print(f\"Cat Probability: {result['cat_probability']:.4f}\")\n",
    "    \n",
    "    # Example 2: Batch prediction on test folder\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"BATCH PREDICTION\")\n",
    "    print(\"=\" * 50)\n",
    "    test_folder = \"DataFiles/test/\"\n",
    "    \n",
    "    if os.path.exists(test_folder):\n",
    "        batch_results = predict_batch(test_folder, model, device, val_test_transforms)\n",
    "        print(f\"\\nProcessed {len(batch_results)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3eafbe-e1e2-442b-bbea-021b27ef8862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a Jupyter cell, run predictions\n",
    "model_path = \"artifacts/cnn_model.pt\"\n",
    "model = load_model(model_path, device)\n",
    "\n",
    "# Single image\n",
    "result = predict_single_image(\"DataFiles/test/1.jpg\", model, device, val_test_transforms)\n",
    "print(result)\n",
    "\n",
    "# Batch\n",
    "results = predict_batch(\"DataFiles/test/\", model, device, val_test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585f0063-f6d3-4f92-a5d3-eb899133b746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25ed4c5-4e06-4b1a-991b-96c17c4c3765",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5082f982-4d82-4168-8ec8-d71f5fea096f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb11cdee-a23a-492c-810c-6c016a1eb662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0725155-fff6-44cb-b7fc-e840ec61eaa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mlopsAssignment2Env",
   "language": "python",
   "name": ".mlopsassignment2env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
